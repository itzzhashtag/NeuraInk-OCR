{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a28da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN_v4.py - 90%+ EMNIST Character Classifier with Progressive Fine-Tuning\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c452e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m   23/10906\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:18:52\u001b[0m 3s/step - accuracy: 0.0265 - loss: 4.8177"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# ========== Load EMNIST Dataset ==========\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'emnist/byclass',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "label_map = ds_info.features['label'].int2str\n",
    "\n",
    "# ========== Preprocessing ==========\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "# ========== Augmentation ==========\n",
    "augment = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomZoom(0.1, 0.1),\n",
    "    layers.RandomContrast(0.1)\n",
    "])\n",
    "\n",
    "def augment_fn(image, label):\n",
    "    return augment(image), label\n",
    "\n",
    "# ========== Datasets ==========\n",
    "train_ds = ds_train.map(preprocess).map(augment_fn).shuffle(2048).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_ds = ds_test.map(preprocess).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# ========== Compute Class Weights ==========\n",
    "labels_np = np.concatenate([\n",
    "    np.argmax(y, axis=-1)\n",
    "    for _, y in tfds.as_numpy(ds_train.map(preprocess).batch(1024))\n",
    "])\n",
    "\n",
    "class_weights_arr = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(NUM_CLASSES),\n",
    "    y=labels_np\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "\n",
    "# ========== Base Models ==========\n",
    "input_layer = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_effnet = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_layer)\n",
    "base_resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=input_layer)\n",
    "\n",
    "# Freeze all layers initially\n",
    "base_effnet.trainable = False\n",
    "base_resnet.trainable = False\n",
    "\n",
    "# EfficientNet branch\n",
    "x1 = base_effnet.output\n",
    "x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "\n",
    "# ResNet branch\n",
    "x2 = base_resnet.output\n",
    "x2 = layers.GlobalAveragePooling2D()(x2)\n",
    "\n",
    "# Merge branches\n",
    "combined = layers.Concatenate()([x1, x2])\n",
    "combined = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(combined)\n",
    "combined = layers.BatchNormalization()(combined)\n",
    "combined = layers.Dropout(0.5)(combined)\n",
    "output = layers.Dense(NUM_CLASSES, activation='softmax')(combined)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# ========== Compile (Initial Training) ==========\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0003),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ========== Callbacks ==========\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(patience=2, factor=0.3, min_lr=1e-6),\n",
    "    ModelCheckpoint(\"EMNIST_V3_best_model.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# ========== Initial Training ==========\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# ========== Save Initial History ==========\n",
    "with open(\"emnist_V3_byclass_stage1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# ========== Fine-Tuning ==========\n",
    "# Unfreeze last N layers of both models\n",
    "N_EFFNET = 20\n",
    "N_RESNET = 30\n",
    "\n",
    "for layer in base_effnet.layers[-N_EFFNET:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in base_resnet.layers[-N_RESNET:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ========== Fine-Tune Training ==========\n",
    "fine_tune_history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# ========== Save Fine-Tune History ==========\n",
    "with open(\"emnist_V3_byclass_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fine_tune_history.history, f)\n",
    "\n",
    "# ========== Evaluation ==========\n",
    "preds = model.predict(test_ds)\n",
    "y_true = np.argmax(np.concatenate([y for _, y in test_ds], axis=0), axis=1)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# ========== Confusion Matrix ==========\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# ========== Visualize Predictions ==========\n",
    "for images, labels in test_ds.take(1):\n",
    "    pred_probs = model.predict(images)\n",
    "    for i in range(25):\n",
    "        idx = random.randint(0, len(images)-1)\n",
    "        img = images[idx].numpy()\n",
    "        true_lbl = label_map(np.argmax(labels[idx].numpy()))\n",
    "        pred_lbl = label_map(np.argmax(pred_probs[idx]))\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"T:{true_lbl}\\nP:{pred_lbl}\")\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
